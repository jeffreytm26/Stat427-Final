---
title: "sml-climate-eda"
author: "Gracie Abrahams"
format: html
editor: visual
---

# Climate Change Through Classification Modeling

## Can we classify extreme weather events using machine learning?

Load necessary packages.

```{r}
library(tidyverse)
library(lubridate)
library(glmnet)
library(pls)
library(ggplot2)
library(ISLR2)
library(leaps)
library(boot)
library(class)
```

## Exploratory Data Analysis

```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
climate_data <- read.csv("../data/Climate-Data.csv")
head(climate_data)
```

```{r}
# Make sure valid_time is a datetime object
climate_data$valid_time <- ymd_hms(climate_data$valid_time)

# Filter between 2000 and 2025
climate_data <- climate_data %>%
  filter(year(valid_time) >= 2000, year(valid_time) <= 2025)

# Check how much is left
nrow(climate_data)

```

```{r}
climate_data <- climate_data %>%
  select(-sst)

```

```{r}
climate_data <- climate_data %>%
  mutate(
    date = as.Date(valid_time),
    year = year(valid_time),
    month = month(valid_time),
    day = day(valid_time),
    hour = hour(valid_time)
  )

```

```{r}
climate_data <- climate_data %>%
  mutate(
    t2m_f = (t2m - 273.15) * 9/5 + 32,
    d2m_f = (d2m - 273.15) * 9/5 + 32
  )

```

```{r}
avg_monthly_temp <- climate_data %>%
  group_by(month) %>%
  summarise(avg_temp_f = mean(t2m_f, na.rm = TRUE))

print(avg_monthly_temp)

```

```{r}
# Check missing values
colSums(is.na(climate_data))

# Drop or impute if needed (for now, drop rows with NAs)
weather_clean <- climate_data %>% drop_na()

```

```{r}
# Temperature distribution
ggplot(weather_clean, aes(x = t2m_f)) +
  geom_histogram(bins = 50, fill = "skyblue") +
  labs(title = "Temperature (F) Distribution")

# Precipitation
ggplot(weather_clean, aes(x = tp)) +
  geom_histogram(bins = 50, fill = "lightgreen") +
  labs(title = "Precipitation Distribution")

# Wind speed (calculated)
weather_clean <- weather_clean %>%
  mutate(wind_speed = sqrt(u10^2 + v10^2))

ggplot(weather_clean, aes(x = wind_speed)) +
  geom_histogram(bins = 50, fill = "salmon") +
  labs(title = "Wind Speed Distribution")

```

```{r}
ggplot(weather_clean, aes(x = valid_time, y = t2m_f)) +
  geom_line(alpha = 0.3) +
  labs(title = "Temperature Over Time")

ggplot(weather_clean, aes(x = valid_time, y = tp)) +
  geom_line(alpha = 0.3) +
  labs(title = "Precipitation Over Time")

```

```{r}
numeric_cols <- weather_clean %>%
  select(t2m_f, d2m_f, msl, sp, wind_speed, tp)

library(corrplot)
corrplot(cor(numeric_cols), method = "circle")

```

```{r}
summary(climate_data)


```

```{r}
# Calculate total wind speed in m/s
weather_clean <- weather_clean %>%
  mutate(
    wind_speed = sqrt(u10^2 + v10^2),  # Calculate total wind speed in m/s
    wind_speed_kmh = wind_speed * 3.6   # Convert wind speed to km/h
  )

# Check the first few rows to confirm the calculation
head(weather_clean)

```

```{r}
# Calculate monthly average temperatures by year
monthly_avg_temp <- climate_data %>%
  group_by(year, month) %>%
  summarise(avg_temp = mean(t2m_f, na.rm = TRUE))

# Plot the monthly averages for each year
ggplot(monthly_avg_temp, aes(x = year, y = avg_temp, color = factor(month))) +
  geom_line() +
  labs(title = "Average Monthly Temperature Over the Years",
       x = "Year", y = "Average Temperature (°F)") +
  theme_minimal() +
  scale_color_brewer(palette = "Set3")

```

```{r}
# Filter out 2025 (incomplete data)
annual_avg_temp <- climate_data %>%
  filter(year != 2025) %>%
  group_by(year) %>%
  summarise(avg_temp = mean(t2m_f, na.rm = TRUE))

# Plot the annual temperature trends
ggplot(annual_avg_temp, aes(x = year, y = avg_temp)) +
  geom_line() +
  labs(title = "Annual Average Temperature Trends (Excluding 2025)",
       x = "Year", y = "Average Temperature (°F)") +
  theme_minimal()


```

```{r}
# Filter for June and December only
june_december_data <- climate_data %>%
  filter(month == 7 | month == 12)

# Calculate average temperature for June and December across years
june_december_avg_temp <- june_december_data %>%
  group_by(year, month) %>%
  summarise(avg_temp = mean(t2m_f, na.rm = TRUE))

# Plot the temperature trends for June and December
ggplot(june_december_avg_temp, aes(x = year, y = avg_temp, color = factor(month))) +
  geom_line() +
  labs(title = "Temperature Trends in June and December",
       x = "Year", y = "Average Temperature (°F)") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red"), labels = c("June", "December"))

```

```{r}
# Remove rows with missing values for temperature
yearly_temp_stats <- climate_data %>%
  filter(year != 2025) %>%
  filter(!is.na(t2m_f)) %>%  # Remove rows with missing temperature data
  group_by(year) %>%
  summarise(
    avg_temp = mean(t2m_f, na.rm = TRUE),
    min_temp = min(t2m_f, na.rm = TRUE),
    max_temp = max(t2m_f, na.rm = TRUE)
  )

# Check if any missing values were present
summary(yearly_temp_stats)

# Plot the min, max, and avg temperature trends by year
ggplot(yearly_temp_stats, aes(x = factor(year))) +
  geom_bar(aes(y = avg_temp), stat = "identity", fill = "lightblue", alpha = 0.6) +
  geom_line(aes(y = min_temp), color = "blue", size = 1, group = 1) +
  geom_line(aes(y = max_temp), color = "red", size = 1, group = 1) +
  labs(title = "Yearly Temperature Trends (Min, Max, and Average)",
       x = "Year", y = "Temperature (°F)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(breaks = seq(min(yearly_temp_stats$min_temp), max(yearly_temp_stats$max_temp), by = 2))  # Adjust y-axis dynamically

```

```{r}
summary(weather_clean$tp)
```

## Extreme Weather Encoding / Classification

```{r}
weather_classified <- weather_clean %>%
  mutate(
    extreme_type = case_when(
      t2m_f >= 90 ~ "Extreme Heat",
      t2m_f <= 10 ~ "Extreme Cold",
      wind_speed_kmh >= 30 ~ "High Wind",
      tp > 0.005 ~ "Heavy Precipitation",
      TRUE ~ "Normal"
    )
  )

```

```{r}

# Count the number of each extreme type by year
extreme_counts <- weather_classified %>%
  group_by(year, extreme_type) %>%
  summarize(count = n(), .groups = 'drop')

# Plot
ggplot(extreme_counts, aes(x = year, y = count, fill = extreme_type)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title = "Extreme Weather Events Per Year",
    x = "Year",
    y = "Number of Events",
    fill = "Extreme Weather Type"
  ) +
  theme_minimal()

```

```{r}
 # Only keep rows that are classified as extreme (not "normal")
extreme_only <- weather_classified %>%
  filter(extreme_type != "Normal")

# Count by year and type
extreme_counts <- extreme_only %>%
  group_by(year, extreme_type) %>%
  summarize(count = n(), .groups = 'drop')

# Plot only extreme events
ggplot(extreme_counts, aes(x = year, y = count, fill = extreme_type)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title = "Extreme Weather Events Per Year (Excludes Normal)",
    x = "Year",
    y = "Number of Events",
    fill = "Extreme Type"
  ) +
  theme_minimal()

```

```{r}
# Total count of each extreme weather type
overall_extreme_summary <- extreme_only %>%
  group_by(extreme_type) %>%
  summarize(total_count = n(), .groups = "drop")

# View the summary
overall_extreme_summary


```

```{r}

# Count number of extreme events per year
extreme_counts_by_year <- weather_classified %>%
  filter(extreme_type != "Normal") %>%
  group_by(year, extreme_type) %>%
  summarise(count = n(), .groups = "drop")

# Plot
ggplot(extreme_counts_by_year, aes(x = year, y = count, fill = extreme_type)) +
  geom_col(position = "dodge") +
  labs(
    title = "Extreme Weather Events by Year",
    x = "Year",
    y = "Count of Extreme Events",
    fill = "Extreme Type"
  ) +
  theme_minimal()

```

```{r}
# Encoding 0 and 1 for each extreme weather type
weather_classified_binary <- weather_classified %>%
  mutate(
    is_extreme_cold = ifelse(extreme_type == "Extreme Cold", 1, 0),
    is_extreme_heat = ifelse(extreme_type == "Extreme Heat", 1, 0),
    is_heavy_precipitation = ifelse(extreme_type == "Heavy Precipitation", 1, 0),
    is_high_wind = ifelse(extreme_type == "High Wind", 1, 0),
    is_normal = ifelse(extreme_type == "Normal", 1, 0)  # Optional: If you want to track normal events too
  )

# Summarize the data to count the number of occurrences for each weather classification
extreme_summary <- weather_classified_binary %>%
  summarise(
    Extreme_Cold = sum(is_extreme_cold),
    Extreme_Heat = sum(is_extreme_heat),
    Heavy_Precipitation = sum(is_heavy_precipitation),
    High_Wind = sum(is_high_wind),
    Normal = sum(is_normal)
  )

# Print the summarized table
extreme_summary

```

## Method 1: KNN Modeling

```{r}

weather_data <- weather_classified_binary 


weather_data$extreme_type <- as.factor(weather_data$extreme_type)

X <- weather_data %>%
  select(where(is.numeric))  

y <- weather_data$extreme_type

set.seed(123)
train_idx <- sample(nrow(weather_data), 0.5 * nrow(weather_data))
Xtrain <- X[train_idx, ]
Xtest <- X[-train_idx, ]
ytrain <- y[train_idx]
ytest <- y[-train_idx]


```

```{r}
library(FNN)
knn_pred <- knn(train = Xtrain, test = Xtest, cl = ytrain, k = 3)

```

```{r}

conf_matrix <- table(Predicted = knn_pred, Actual = ytest)

sensitivity_per_class <- diag(conf_matrix) / rowSums(conf_matrix)
specificity_per_class <- (rowSums(conf_matrix) - diag(conf_matrix)) / rowSums(conf_matrix)

balanced_accuracy <- mean((sensitivity_per_class + specificity_per_class) / 2)
print(paste("Balanced Accuracy:", round(balanced_accuracy, 2)))

```

```{r}
accuracy_knn <- sum(diag(conf_matrix)) / sum(conf_matrix)
round(accuracy_knn, 4)


```

Make 2 groups instead of each extreme weather classification (Normal & Extreme Weather)

Use KNN (K=3) to classify extreme weather

```{r}
weather_classified_binary$extreme_weather_binary <- ifelse(weather_classified_binary$extreme_type %in% 
    c("Extreme Cold", "Extreme Heat", "Heavy Precipitation", "High Wind"), "Extreme Weather", "Normal")


y <- weather_classified_binary$extreme_weather_binary


set.seed(123)
train_idx <- sample(nrow(weather_classified_binary), 0.5 * nrow(weather_classified_binary))
Xtrain <- X[train_idx, ]
Xtest <- X[-train_idx, ]
ytrain <- y[train_idx]
ytest <- y[-train_idx]

knn_pred <- knn(train = Xtrain, test = Xtest, cl = ytrain, k = 3)


```

```{r}
conf_matrix <- table(Predicted = knn_pred, Actual = ytest)
conf_matrix
```

```{r}
accuracy <- sum(knn_pred == ytest) / length(ytest)
accuracy
```

**True Positives (TP)** = 1246 (correctly predicted extreme)

**False Positives (FP)** = 298 (predicted extreme, actually normal)

**False Negatives (FN)** = 1400 (predicted normal, actually extreme)

**True Negatives (TN)** = 107996 (correctly predicted normal)

Here we see an extremely high accuracy, which is due to a class imbalance. Because of this, we should try again with more balanced classes.

```{r}
#Undersample 'Normal' to match number of 'Extreme Weather' cases
normal <- weather_classified_binary %>% filter(extreme_weather_binary == "Normal")
extreme <- weather_classified_binary %>% filter(extreme_weather_binary == "Extreme Weather")

set.seed(123)
normal_sample <- normal %>% sample_n(nrow(extreme))

balanced_data <- bind_rows(extreme, normal_sample)

```

```{r}
#KNN Model wih balanced data 

X <- balanced_data %>% select(where(is.numeric))
y <- balanced_data$extreme_weather_binary

set.seed(123)
train_idx <- sample(nrow(balanced_data), 0.7 * nrow(balanced_data))
Xtrain <- X[train_idx, ]
Xtest <- X[-train_idx, ]
ytrain <- y[train_idx]
ytest <- y[-train_idx]


knn_pred <- knn(train = Xtrain, test = Xtest, cl = ytrain, k = 3)

conf_matrix <- table(Predicted = knn_pred, Actual = ytest)
print(conf_matrix)

accuracy <- sum(knn_pred == ytest) / length(ytest)
accuracy

```

```{r}
#Find best K
classification_rates <- rep(NA, 20)

for (k in 1:20) {
  knn_predictions <- knn(train = Xtrain, test = Xtest, cl = ytrain, k = k)
  classification_rates[k] <- mean(knn_predictions == ytest)

}


k_3_rate <- classification_rates[3]


better_k <- which(classification_rates > k_3_rate)


classification_rates

```

```{r}
plot(1:20, classification_rates, type = "b", pch = 18, col = "blue",
     xlab = "K Value", ylab = "Classification Rate",
     main = "K vs. Classification Rate")
```

Here we see that as we increase K, the classification rate decreases, which we can expect due to what we have referred to as the "curse of dimensionality" which is when the number of features p is large there tends to be a deterioration in the performance of KNN.

## Method 2: Logistic Regression

```{r}
# Logistic regression model to predict extreme weather (binary)
logit_data <- weather_classified_binary %>%
  mutate(extreme_weather_binary = factor(extreme_weather_binary))

X <- logit_data %>% 
  select(t2m_f, d2m_f, wind_speed_kmh, msl, sp, tp)

y <- logit_data$extreme_weather_binary

logit_df <- cbind(X, extreme_weather_binary = y)

logit_model <- glm(extreme_weather_binary ~ ., data = logit_df, family = binomial)

summary(logit_model)

```

```{r}
# Generate predictions and evaluate model accuracy
logit_probs <- predict(logit_model, type = "response")
logit_preds <- ifelse(logit_probs > 0.5, "Extreme Weather", "Normal")
logit_preds <- factor(logit_preds, levels = levels(y))

conf_matrix <- table(Predicted = logit_preds, Actual = y)
conf_matrix

mean(logit_preds == y)
```

The logistic regression model shows significant class imbalance, with the model predicting nearly all cases as "Extreme Weather." Despite strong statistical significance in the coefficients, the overall accuracy is only 1.5%, indicating poor predictive performance. This suggests that the model is not effectively distinguishing between normal and extreme weather, and rebalancing the dataset or applying class weights is necessary for better classification.

```{r}
# Rebalance dataset by downsampling the majority class
extreme <- logit_df %>% filter(extreme_weather_binary == "Extreme Weather")
normal <- logit_df %>% filter(extreme_weather_binary == "Normal") %>% sample_n(nrow(extreme))
balanced_df <- bind_rows(extreme, normal)

# Fit logistic regression model on balanced data
logit_model_bal <- glm(extreme_weather_binary ~ ., data = balanced_df, family = binomial)

# Generate predictions and evaluate
logit_probs <- predict(logit_model_bal, type = "response")
logit_preds <- ifelse(logit_probs > 0.5, "Extreme Weather", "Normal")
logit_preds <- factor(logit_preds, levels = levels(balanced_df$extreme_weather_binary))
conf_matrix <- table(Predicted = logit_preds, Actual = balanced_df$extreme_weather_binary)
conf_matrix
mean(logit_preds == balanced_df$extreme_weather_binary)

```

After rebalancing the dataset to address class imbalance, the logistic regression model still performed poorly, with only 6.4% accuracy. The confusion matrix shows that the model continues to misclassify most observations, suggesting that the relationship between predictors and extreme weather is likely non-linear or too complex for logistic regression to capture. This indicates that more flexible models, such as QDA, LDA, or tree-based methods, may be better suited for this classification task.

## Method 3: QDA/LDA

```{r}
library(MASS)

# Fit LDA
lda_model <- lda(extreme_weather_binary ~ ., data = balanced_df)
lda_preds <- predict(lda_model)$class
lda_acc <- mean(lda_preds == balanced_df$extreme_weather_binary)

# Fit QDA
qda_model <- qda(extreme_weather_binary ~ ., data = balanced_df)
qda_preds <- predict(qda_model)$class
qda_acc <- mean(qda_preds == balanced_df$extreme_weather_binary)

# Output accuracies
lda_acc
qda_acc

```

The LDA and QDA models performed significantly better than logistic regression, with classification accuracies of approximately **91.97%** and **93.99%**, respectively. This improvement suggests that the relationship between features and extreme weather events is non-linear and better captured by flexible, distribution-based models like LDA and QDA. These results indicate that QDA, in particular, may be a strong candidate for classifying extreme weather in this dataset.

## Linear Regression

```{r}
# Select Data
regression_df <- weather_clean %>%
  dplyr::select(t2m, month, d2m_f, wind_speed_kmh, msl, sp, tp) %>%
  drop_na()
```

```{r}
lm_model <- lm(t2m ~ ., data = regression_df)
summary(lm_model)
```

The linear regression model achieved a high R² of 0.9165, indicating that over 91% of the variation in temperature (`t2m`) is explained by the selected predictors. All predictors were statistically significant, and the residual standard error was relatively low at 2.93, suggesting a good overall fit.

## Polynomial Regression

```{r}
poly_model <- lm(t2m ~ poly(month, 3), data = regression_df)
summary(poly_model)
```

The polynomial regression model used a 3rd-degree transformation of the `month` variable to model seasonal trends in temperature. While all terms were significant, the R² was lower at 0.6622, and the residual error was larger compared to the full linear model. This suggests that `month` alone, even with polynomial terms, is insufficient for accurate temperature prediction.

## Ridge/Lasso

```{r}
library(glmnet)

# Prepare data
X <- model.matrix(t2m ~ ., regression_df)[, -1]
y <- regression_df$t2m

# Ridge Regression (alpha = 0)
cv_ridge <- cv.glmnet(X, y, alpha = 0)
ridge_best_lambda <- cv_ridge$lambda.min
ridge_preds <- predict(cv_ridge, s = ridge_best_lambda, newx = X)
ridge_rmse <- sqrt(mean((ridge_preds - y)^2))
ridge_rmse

# Lasso Regression (alpha = 1)
cv_lasso <- cv.glmnet(X, y, alpha = 1)
lasso_best_lambda <- cv_lasso$lambda.min
lasso_preds <- predict(cv_lasso, s = lasso_best_lambda, newx = X)
lasso_rmse <- sqrt(mean((lasso_preds - y)^2))
lasso_rmse

# Coefficients
coef(cv_ridge, s = "lambda.min")
coef(cv_lasso, s = "lambda.min")

```

Ridge regression, which shrinks coefficients but retains all predictors, produced an RMSE of 4.65. While this is worse than the linear and lasso models, ridge was likely affected by collinearity among predictors and over-smoothing due to the penalty term.

Lasso regression yielded the best performance among the penalized methods, with an RMSE of 2.94 — very close to the unregularized linear model. It also performed variable selection, shrinking some coefficients more aggressively (e.g., `tp`) while still retaining key predictors, making it a strong choice for both accuracy and interpretability.

## Regression Models

```{r}
library(glmnet)
library(ggplot2)

# Prepare data
regression_df <- weather_clean %>%
  dplyr::select(t2m, month, d2m_f, wind_speed_kmh, msl, sp, tp) %>%
  drop_na()

X <- model.matrix(t2m ~ ., regression_df)[, -1]
y <- regression_df$t2m

# 1. Linear Regression
lm_model <- lm(t2m ~ ., data = regression_df)
lm_preds <- predict(lm_model)

# 2. Polynomial Regression (month as poly term)
poly_model <- lm(t2m ~ poly(month, 3), data = regression_df)
poly_preds <- predict(poly_model)

# 3. Ridge Regression
cv_ridge <- cv.glmnet(X, y, alpha = 0)
ridge_preds <- predict(cv_ridge, s = "lambda.min", newx = X)

# 4. Lasso Regression
cv_lasso <- cv.glmnet(X, y, alpha = 1)
lasso_preds <- predict(cv_lasso, s = "lambda.min", newx = X)

# Create long-format dataframe for plotting
plot_df <- data.frame(
  Actual = y,
  Linear = lm_preds,
  Polynomial = poly_preds,
  Ridge = as.vector(ridge_preds),
  Lasso = as.vector(lasso_preds)
) %>%
  pivot_longer(-Actual, names_to = "Model", values_to = "Predicted")

# Plot predicted vs. actual
ggplot(plot_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.3, size = 0.8) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  facet_wrap(~Model, scales = "free") +
  labs(title = "Predicted vs. Actual Temperature by Model",
       x = "Actual Temperature (K)", y = "Predicted Temperature (K)") +
  theme_minimal()
```

## Evaluation and Cross Validation

```{r}

# Classification model comparison
classification_results <- data.frame(
  Model = c("KNN (Unbalanced)", "KNN (Balanced)", "Logistic Regression", "LDA", "QDA"),
  Accuracy = c(0.985, 0.888, 0.064, 0.9197, 0.9399),
  Balanced_Accuracy = c(0.50, NA, NA, NA, NA),
  Notes = c(
    "Overestimates due to class imbalance",
    "More realistic; tuned with CV",
    "Underperforms despite significance",
    "Strong separation with linear boundaries",
    "Best classifier; handles non-linearity"
  )
)

kable(classification_results, caption = "Classification Model Comparison")

# Regression model comparison
regression_results <- data.frame(
  Model = c("Linear Regression", "Polynomial Regression", "Ridge Regression", "Lasso Regression"),
  RMSE = c(2.93, 5.88, 4.65, 2.94),
  Adjusted_R2 = c(0.9165, 0.6622, NA, NA),
  Notes = c(
    "Best overall fit; all predictors significant",
    "Captures seasonality; limited features",
    "Regularized; retains all predictors",
    "Sparse solution; similar to linear"
  )
)

kable(regression_results, caption = "Regression Model Comparison")
```

We evaluated multiple classification methods to predict extreme weather events, including K-Nearest Neighbors (KNN), logistic regression, LDA, and QDA. KNN initially appeared highly accurate (\>98%), but deeper inspection revealed this was due to severe class imbalance. After balancing the data, KNN's performance became more realistic, achieving \~88% accuracy and a balanced accuracy of 0.5. Logistic regression struggled even after rebalancing, with only 6.4% accuracy, indicating it could not capture complex decision boundaries. LDA and QDA, by contrast, performed significantly better, with classification accuracies of 91.97% and 93.99% respectively, showing that distribution-based models can better handle the underlying structure of extreme weather classification. Cross-validation was used to select the optimal number of neighbors in KNN and to ensure fair model comparison. These results highlight the importance of both class balance and cross-validation in evaluating classifier performance in imbalanced real-world datasets.

To evaluate model performance, we used RMSE (Root Mean Squared Error) and adjusted R² across four regression approaches: linear, polynomial, ridge, and lasso. The standard linear model performed best overall, with an adjusted R² of 0.9165 and a low residual error, indicating a strong linear relationship between temperature and the selected predictors. Polynomial regression, despite modeling seasonal trends with `month`, showed weaker performance due to its limited feature scope. Ridge regression used cross-validation to select the optimal penalty parameter (`λ`), but still underperformed with an RMSE of 4.65. Lasso regression also used cross-validation for tuning and achieved a much lower RMSE of 2.94 by automatically selecting relevant features. These results underscore the effectiveness of regularized models when handling collinearity or when model simplicity is preferred. Cross-validation played a key role in objectively comparing flexibility and prediction error, helping avoid overfitting.
