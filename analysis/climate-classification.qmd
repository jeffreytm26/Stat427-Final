---
title: "Predicting Temperature Changes Over Time to Indicate Climate Trends"
author: "Gracie Abrahams, Jeffrey Mann, Luke Carignan"
format: html
editor: visual
---

# Predicting Temperature Changes Over Time to Indicate Climate Trends

## 1) Can we classify extreme weather events using machine learning?

Load necessary packages.

```{r}
library(dplyr)
library(tidyverse)
library(lubridate)
library(glmnet)
library(pls)
library(ggplot2)
library(ISLR2)
library(leaps)
library(corrplot)
library(boot)
library(class)
library(MASS)
library(GGally)
```

## Exploratory Data Analysis

```{r}
climate_data <- read.csv("../data/Climate-Data.csv")
head(climate_data)
```

```{r}

climate_data$valid_time <- ymd_hms(climate_data$valid_time)

# Filter between 2000 and 2025
climate_data <- climate_data %>%
  filter(year(valid_time) >= 2000, year(valid_time) <= 2025)

nrow(climate_data)

```

```{r}
library(dplyr)
climate_data <- climate_data %>% dplyr::select(-sst)

```

```{r}
climate_data <- climate_data %>%
  mutate(
    date = as.Date(valid_time),
    year = year(valid_time),
    month = month(valid_time),
    day = day(valid_time),
    hour = hour(valid_time)
  )

```

```{r}
climate_data <- climate_data %>%
  mutate(
    t2m_f = (t2m - 273.15) * 9/5 + 32,
    d2m_f = (d2m - 273.15) * 9/5 + 32
  )

```

```{r}
avg_monthly_temp <- climate_data %>%
  group_by(month) %>%
  summarise(avg_temp_f = mean(t2m_f, na.rm = TRUE))

avg_monthly_temp

```

```{r}

colSums(is.na(climate_data))

weather_clean <- climate_data %>% drop_na()

```

```{r}
# Temperature distribution
ggplot(weather_clean, aes(x = t2m_f)) +
  geom_histogram(bins = 50, fill = "skyblue") +
  labs(title = "Temperature (F) Distribution")

# Precipitation
ggplot(weather_clean, aes(x = tp)) +
  geom_histogram(bins = 50, fill = "lightgreen") +
  labs(title = "Precipitation Distribution")

# Wind speed (calculated)
weather_clean <- weather_clean %>%
  mutate(wind_speed = sqrt(u10^2 + v10^2))

ggplot(weather_clean, aes(x = wind_speed)) +
  geom_histogram(bins = 50, fill = "salmon") +
  labs(title = "Wind Speed Distribution")

```

```{r}
ggplot(weather_clean, aes(x = valid_time, y = t2m_f)) +
  geom_line(alpha = 0.3) +
  labs(title = "Temperature Over Time")

ggplot(weather_clean, aes(x = valid_time, y = tp)) +
  geom_line(alpha = 0.3) +
  labs(title = "Precipitation Over Time")

```

```{r}
numeric_cols <- weather_clean %>%
  dplyr::select(t2m_f, d2m_f, msl, sp, wind_speed, tp)

corrplot(cor(numeric_cols), method = "circle")

```

```{r}
summary(climate_data)


```

```{r}
# Calculate total wind speed in m/s
weather_clean <- weather_clean %>%
  mutate(
    wind_speed = sqrt(u10^2 + v10^2),  # Calculate total wind speed in m/s
    wind_speed_kmh = wind_speed * 3.6   # Convert wind speed to km/h
  )

# Check the first few rows to confirm the calculation
head(weather_clean)

```

```{r}
# Calculate monthly average temperatures by year
monthly_avg_temp <- climate_data %>%
  group_by(year, month) %>%
  summarise(avg_temp = mean(t2m_f, na.rm = TRUE))

# Plot the monthly averages for each year
ggplot(monthly_avg_temp, aes(x = year, y = avg_temp, color = factor(month))) +
  geom_line() +
  labs(title = "Average Monthly Temperature Over the Years",
       x = "Year", y = "Average Temperature (°F)") +
  theme_minimal() +
  scale_color_brewer(palette = "Set3")

```

```{r}
# Filter out 2025 
annual_avg_temp <- climate_data %>%
  filter(year != 2025) %>%
  group_by(year) %>%
  summarise(avg_temp = mean(t2m_f, na.rm = TRUE))

# Plot the annual temperature trends
ggplot(annual_avg_temp, aes(x = year, y = avg_temp)) +
  geom_line() +
  labs(title = "Annual Average Temperature Trends (Excluding 2025)",
       x = "Year", y = "Average Temperature (°F)") +
  theme_minimal()


```

```{r}
# Filter for June and December only
# This is to check that data looks accurate for warm/cool months typically to ensure we have correct coordinates
june_december_data <- climate_data %>%
  filter(month == 7 | month == 12)

# Calculate average temperature for June and December across years
june_december_avg_temp <- june_december_data %>%
  group_by(year, month) %>%
  summarise(avg_temp = mean(t2m_f, na.rm = TRUE))

# Plot the temperature trends for June and December
ggplot(june_december_avg_temp, aes(x = year, y = avg_temp, color = factor(month))) +
  geom_line() +
  labs(title = "Temperature Trends in June and December",
       x = "Year", y = "Average Temperature (°F)") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red"), labels = c("June", "December"))

```

```{r}
# Remove rows with missing values for temperature
yearly_temp_stats <- climate_data %>%
  filter(year != 2025) %>%
  filter(!is.na(t2m_f)) %>%  
  group_by(year) %>%
  summarise(
    avg_temp = mean(t2m_f, na.rm = TRUE),
    min_temp = min(t2m_f, na.rm = TRUE),
    max_temp = max(t2m_f, na.rm = TRUE)
  )

# Check if any missing values were present
summary(yearly_temp_stats)

# Plot the min, max, and avg temperature trends by year
ggplot(yearly_temp_stats, aes(x = factor(year))) +
  geom_bar(aes(y = avg_temp), stat = "identity", fill = "lightblue", alpha = 0.6) +
  geom_line(aes(y = min_temp), color = "blue", size = 1, group = 1) +
  geom_line(aes(y = max_temp), color = "red", size = 1, group = 1) +
  labs(title = "Yearly Temperature Trends (Min, Max, and Average)",
       x = "Year", y = "Temperature (°F)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(breaks = seq(min(yearly_temp_stats$min_temp), max(yearly_temp_stats$max_temp), by = 2)) 

```

```{r}
summary(weather_clean$tp)
```

## Extreme Weather Encoding / Classification

```{r}
weather_classified <- weather_clean %>%
  mutate(
    extreme_type = case_when(
      t2m_f >= 90 ~ "Extreme Heat",
      t2m_f <= 10 ~ "Extreme Cold",
      wind_speed_kmh >= 30 ~ "High Wind",
      tp > 0.005 ~ "Heavy Precipitation",
      TRUE ~ "Normal"
    )
  )

```

```{r}

# Count the number of each extreme type by year
extreme_counts <- weather_classified %>%
  group_by(year, extreme_type) %>%
  summarize(count = n(), .groups = 'drop')

# Plot
ggplot(extreme_counts, aes(x = year, y = count, fill = extreme_type)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title = "Extreme Weather Events Per Year",
    x = "Year",
    y = "Number of Events",
    fill = "Extreme Weather Type"
  ) +
  theme_minimal()

```

```{r}
 # Only keep rows that are classified as extreme, normal has class imbalance
extreme_only <- weather_classified %>%
  filter(extreme_type != "Normal")

# Count by year and type
extreme_counts <- extreme_only %>%
  group_by(year, extreme_type) %>%
  summarize(count = n(), .groups = 'drop')

# Plot only extreme events
ggplot(extreme_counts, aes(x = year, y = count, fill = extreme_type)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title = "Extreme Weather Events Per Year (Excludes Normal)",
    x = "Year",
    y = "Number of Events",
    fill = "Extreme Type"
  ) +
  theme_minimal()

```

```{r}
# Total count of each extreme weather type
overall_extreme_summary <- extreme_only %>%
  group_by(extreme_type) %>%
  summarize(total_count = n(), .groups = "drop")

# View the summary
overall_extreme_summary


```

```{r}

# Count number of extreme events per year
extreme_counts_by_year <- weather_classified %>%
  filter(extreme_type != "Normal") %>%
  group_by(year, extreme_type) %>%
  summarise(count = n(), .groups = "drop")

# Plot
ggplot(extreme_counts_by_year, aes(x = year, y = count, fill = extreme_type)) +
  geom_col(position = "dodge") +
  labs(
    title = "Extreme Weather Events by Year",
    x = "Year",
    y = "Count of Extreme Events",
    fill = "Extreme Type"
  ) +
  theme_minimal()

```

```{r}
# Encoding 0 and 1 for each extreme weather type
weather_classified_binary <- weather_classified %>%
  mutate(
    is_extreme_cold = ifelse(extreme_type == "Extreme Cold", 1, 0),
    is_extreme_heat = ifelse(extreme_type == "Extreme Heat", 1, 0),
    is_heavy_precipitation = ifelse(extreme_type == "Heavy Precipitation", 1, 0),
    is_high_wind = ifelse(extreme_type == "High Wind", 1, 0),
    is_normal = ifelse(extreme_type == "Normal", 1, 0)  
  )

# Summarize the data to count the number of occurrences for each weather classification
extreme_summary <- weather_classified_binary %>%
  summarise(
    Extreme_Cold = sum(is_extreme_cold),
    Extreme_Heat = sum(is_extreme_heat),
    Heavy_Precipitation = sum(is_heavy_precipitation),
    High_Wind = sum(is_high_wind),
    Normal = sum(is_normal)
  )

extreme_summary

```

## Method 1: KNN Modeling

```{r}

weather_data <- weather_classified_binary 


weather_data$extreme_type <- as.factor(weather_data$extreme_type)

X <- weather_data %>%
  dplyr::select(where(is.numeric))  

y <- weather_data$extreme_type

set.seed(123)
train_idx <- sample(nrow(weather_data), 0.5 * nrow(weather_data))
Xtrain <- X[train_idx, ]
Xtest <- X[-train_idx, ]
ytrain <- y[train_idx]
ytest <- y[-train_idx]


```

```{r}
# To speed up runtime
library(FNN)
knn_pred <- knn(train = Xtrain, test = Xtest, cl = ytrain, k = 3)

```

```{r}
# Try balancing
conf_matrix <- table(Predicted = knn_pred, Actual = ytest)

sensitivity_per_class <- diag(conf_matrix) / rowSums(conf_matrix)
specificity_per_class <- (rowSums(conf_matrix) - diag(conf_matrix)) / rowSums(conf_matrix)

balanced_accuracy <- mean((sensitivity_per_class + specificity_per_class) / 2)
round(balanced_accuracy, 2)

```

```{r}
accuracy_knn <- sum(diag(conf_matrix)) / sum(conf_matrix)
round(accuracy_knn, 4)


```

Make 2 groups instead of each extreme weather classification (Normal & Extreme Weather)

Use KNN (K=3) to classify extreme weather

```{r}
weather_classified_binary$extreme_weather_binary <- ifelse(weather_classified_binary$extreme_type %in% 
    c("Extreme Cold", "Extreme Heat", "Heavy Precipitation", "High Wind"), "Extreme Weather", "Normal")


y <- weather_classified_binary$extreme_weather_binary


set.seed(123)
train_idx <- sample(nrow(weather_classified_binary), 0.5 * nrow(weather_classified_binary))
Xtrain <- X[train_idx, ]
Xtest <- X[-train_idx, ]
ytrain <- y[train_idx]
ytest <- y[-train_idx]

knn_pred <- knn(train = Xtrain, test = Xtest, cl = ytrain, k = 3)


```

```{r}
conf_matrix <- table(Predicted = knn_pred, Actual = ytest)
conf_matrix
```

```{r}
accuracy <- sum(knn_pred == ytest) / length(ytest)
accuracy
```

**True Positives (TP)** = 1246 (correctly predicted extreme)

**False Positives (FP)** = 298 (predicted extreme, actually normal)

**False Negatives (FN)** = 1400 (predicted normal, actually extreme)

**True Negatives (TN)** = 107996 (correctly predicted normal)

Here we see an extremely high accuracy, which is due to a class imbalance. Because of this, we should try again with more balanced classes.

```{r}
#Undersample 'Normal' to match number of 'Extreme Weather' cases
normal <- weather_classified_binary %>% filter(extreme_weather_binary == "Normal")
extreme <- weather_classified_binary %>% filter(extreme_weather_binary == "Extreme Weather")

set.seed(123)
normal_sample <- normal %>% sample_n(nrow(extreme))

balanced_data <- bind_rows(extreme, normal_sample)

```

```{r}
#KNN Model wih balanced data 

X <- balanced_data %>% dplyr::select(where(is.numeric))
y <- balanced_data$extreme_weather_binary

set.seed(123)
train_idx <- sample(nrow(balanced_data), 0.7 * nrow(balanced_data))
Xtrain <- X[train_idx, ]
Xtest <- X[-train_idx, ]
ytrain <- y[train_idx]
ytest <- y[-train_idx]


knn_pred <- knn(train = Xtrain, test = Xtest, cl = ytrain, k = 3)

conf_matrix_knn <- table(Predicted = knn_pred, Actual = ytest)
conf_matrix_knn

accuracy <- sum(knn_pred == ytest) / length(ytest)
accuracy

```

```{r}
#Find best K
classification_rates <- rep(NA, 20)

for (k in 1:20) {
  knn_predictions <- knn(train = Xtrain, test = Xtest, cl = ytrain, k = k)
  classification_rates[k] <- mean(knn_predictions == ytest)

}


k_3_rate <- classification_rates[3]


better_k <- which(classification_rates > k_3_rate)


classification_rates

```

```{r}
plot(1:20, classification_rates, type = "b", pch = 18, col = "blue",
     xlab = "K Value", ylab = "Classification Rate",
     main = "K vs. Classification Rate")
```

Here we see that as we increase K, the classification rate decreases, which we can expect due to what we have referred to as the "curse of dimensionality" which is when the number of features p is large there tends to be a deterioration in the performance of KNN.

```{r}
knn_tp <- conf_matrix_knn["Extreme Weather", "Extreme Weather"]
knn_tn <- conf_matrix_knn["Normal", "Normal"]
knn_fp <- conf_matrix_knn["Extreme Weather", "Normal"]
knn_fn <- conf_matrix_knn["Normal", "Extreme Weather"]

knn_precision <- knn_tp / (knn_tp + knn_fp)
knn_recall <- knn_tp / (knn_tp + knn_fn)
knn_f1 <- 2 * (knn_precision * knn_recall) / (knn_precision + knn_recall)

knn_recall
knn_precision
knn_f1
```

## Method 2: Logistic Regression

```{r}
# Logistic regression model to predict extreme weather 
logit_data <- weather_classified_binary %>%
  mutate(extreme_weather_binary = factor(extreme_weather_binary))

X <- logit_data %>% 
  dplyr::select(t2m_f, d2m_f, wind_speed_kmh, msl, sp, tp)

y <- logit_data$extreme_weather_binary

logit_df <- cbind(X, extreme_weather_binary = y)

logit_model <- glm(extreme_weather_binary ~ ., data = logit_df, family = binomial)

summary(logit_model)

```

```{r}
# Generate predictions and evaluate model accuracy
logit_probs <- predict(logit_model, type = "response")
logit_preds <- ifelse(logit_probs > 0.5, "Extreme Weather", "Normal")
logit_preds <- factor(logit_preds, levels = levels(y))

conf_matrix <- table(Predicted = logit_preds, Actual = y)
conf_matrix

mean(logit_preds == y)
```

The logistic regression model shows significant class imbalance, with the model predicting nearly all cases as "Extreme Weather." Despite strong statistical significance in the coefficients, the overall accuracy is only 1.5%, indicating poor predictive performance. This suggests that the model is not effectively distinguishing between normal and extreme weather, and rebalancing the dataset or applying class weights is necessary for better classification.

```{r}
# Rebalance dataset by downsampling the majority class
extreme <- logit_df %>% filter(extreme_weather_binary == "Extreme Weather")
normal <- logit_df %>% filter(extreme_weather_binary == "Normal") %>% sample_n(nrow(extreme))
balanced_df <- bind_rows(extreme, normal)

# Fit logistic regression model on balanced data
logit_model_bal <- glm(extreme_weather_binary ~ ., data = balanced_df, family = binomial)

# Generate predictions and evaluate
logit_probs <- predict(logit_model_bal, type = "response")
logit_preds <- ifelse(logit_probs > 0.5, "Extreme Weather", "Normal")
logit_preds <- factor(logit_preds, levels = levels(balanced_df$extreme_weather_binary))
conf_matrix_logit <- table(Predicted = logit_preds, Actual = balanced_df$extreme_weather_binary)
conf_matrix_logit
mean(logit_preds == balanced_df$extreme_weather_binary)

```

```{r}
logit_tp <- conf_matrix_logit["Extreme Weather", "Extreme Weather"]
logit_tn <- conf_matrix_logit["Normal", "Normal"]
logit_fp <- conf_matrix_logit["Extreme Weather", "Normal"]
logit_fn <- conf_matrix_logit["Normal", "Extreme Weather"]

logit_precision <- logit_tp / (logit_tp + logit_fp)
logit_recall <- logit_tp / (logit_tp + logit_fn)
logit_f1 <- 2 * (logit_precision * logit_recall) / (logit_precision + logit_recall)

logit_recall
logit_precision
logit_f1
```

After rebalancing the dataset to address class imbalance, the logistic regression model still performed poorly, with only 6.4% accuracy. The confusion matrix shows that the model continues to misclassify most observations, suggesting that the relationship between predictors and extreme weather is likely non-linear or too complex for logistic regression to capture. This indicates that more flexible models, such as QDA, LDA, or tree-based methods, may be better suited for this classification task.

```{r}
#ROC Curve and Error Evaluation 
prob <- predict(logit_model_bal, type = "response")
actual <- ifelse(balanced_df$extreme_weather_binary == "Extreme Weather", 1, 0)


threshold <- seq(0, 1, 0.01)
TPR <- FPR <- err.rate <- rep(0, length(threshold))


for (i in seq_along(threshold)) {
  Yhat <- ifelse(prob >= threshold[i], 1, 0)
  
  err.rate[i] <- mean(Yhat != actual)
  
  TPR[i] <- sum(Yhat == 1 & actual == 1) / sum(actual == 1)
  FPR[i] <- sum(Yhat == 1 & actual == 0) / sum(actual == 0)
}


ggplot(tibble(threshold, err.rate), aes(threshold, err.rate)) + 
  geom_point() +
  labs(x = "Threshold", y = "Error Rate", title = "Error Rate vs. Threshold") +
  theme_minimal()


best_threshold <- threshold[which.min(err.rate)]
worst_threshold <- threshold[which.max(err.rate)]

best_TPR <- TPR[which.min(err.rate)]
best_FPR <- FPR[which.min(err.rate)]
worst_TPR <- TPR[which.max(err.rate)]
worst_FPR <- FPR[which.max(err.rate)]


ggplot(tibble(TPR, FPR), aes(FPR, TPR)) + 
  geom_point() + 
  geom_abline(color = "red", lty = 2) +
  labs(x = "False Positive Rate", y = "True Positive Rate", title = "ROC Curve") +
  theme_minimal()

best_threshold

```

As we mentioned before, and reemphasized by what we see here with the ROC curve and error rate graphs, it does not seem Logistic Regression is a strong method to fit our data. Especially given this upside own U shape we see, that possibly suggests that the relationship between the predictors and the outcome is more complex than logistic regression can handle. We may need more flexible models that can capture this data.

## Method 3: QDA/LDA

```{r}

# LDA
lda_model <- lda(extreme_weather_binary ~ ., data = balanced_df)
lda_preds <- predict(lda_model)$class
lda_acc <- mean(lda_preds == balanced_df$extreme_weather_binary)

# QDA
qda_model <- qda(extreme_weather_binary ~ ., data = balanced_df)
qda_preds <- predict(qda_model)$class
qda_acc <- mean(qda_preds == balanced_df$extreme_weather_binary)

# Output accuracies
lda_acc
qda_acc

```

The LDA and QDA models performed significantly better than logistic regression, with classification accuracies of approximately **91.97%** and **93.99%**, respectively. This improvement suggests that the relationship between features and extreme weather events is non-linear and better captured by flexible, distribution-based models like LDA and QDA. These results indicate that QDA, in particular, may be a strong candidate for classifying extreme weather in this dataset. This aligns with our previous findings, given that QDA is a more flexible model and may be able to capture our data better than other models.

```{r}
# GGpairs 
ggpairs(balanced_df, columns = 1:(ncol(balanced_df)-1), 
        aes(color = extreme_weather_binary, alpha = 0.5))

```

```{r}
# Confusion Matrices for LDA And QDA
lda_conf_matrix <- table(Predicted = lda_preds, Actual = balanced_df$extreme_weather_binary)
lda_conf_matrix

qda_conf_matrix <- table(Predicted = qda_preds, Actual = balanced_df$extreme_weather_binary)
qda_conf_matrix

```

```{r}
#More metrics
lda_tp <- lda_conf_matrix["Extreme Weather", "Extreme Weather"]
lda_tn <- lda_conf_matrix["Normal", "Normal"]
lda_fp <- lda_conf_matrix["Extreme Weather", "Normal"]
lda_fn <- lda_conf_matrix["Normal", "Extreme Weather"]

lda_precision <- lda_tp / (lda_tp + lda_fp)
lda_recall <- lda_tp / (lda_tp + lda_fn)
lda_f1 <- 2 * (lda_precision * lda_recall) / (lda_precision + lda_recall)


qda_tp <- qda_conf_matrix["Extreme Weather", "Extreme Weather"]
qda_tn <- qda_conf_matrix["Normal", "Normal"]
qda_fp <- qda_conf_matrix["Extreme Weather", "Normal"]
qda_fn <- qda_conf_matrix["Normal", "Extreme Weather"]

qda_precision <- qda_tp / (qda_tp + qda_fp)
qda_recall <- qda_tp / (qda_tp + qda_fn)
qda_f1 <- 2 * (qda_precision * qda_recall) / (qda_precision + qda_recall)

lda_precision
lda_recall
lda_f1
qda_precision
qda_recall
qda_f1

```

```{r}
# Get predicted probabilities for the Extreme Weather class
lda_probs <- predict(lda_model, newdata = balanced_df)$posterior[, "Extreme Weather"]
qda_probs <- predict(qda_model, newdata = balanced_df)$posterior[, "Extreme Weather"]

```

```{r}

true_labels <- factor(balanced_df$extreme_weather_binary, levels = c("Normal", "Extreme Weather"))
positive_class <- "Extreme Weather"

#LDA
threshold_lda <- seq(0, 1, 0.01)
TPR_lda <- FPR_lda <- err_rate_lda <- rep(0, length(threshold_lda))
```

```{r}
for (i in seq_along(threshold_lda)) {
  Yhat_lda <- ifelse(lda_probs >= threshold_lda[[i]], positive_class, "Normal")
  Yhat_lda_factor <- factor(Yhat_lda, levels = c("Normal", "Extreme Weather"))
  err_rate_lda[i] <- mean(Yhat_lda_factor != true_labels)
  TPR_lda[i] <- sum(Yhat_lda == positive_class & true_labels == positive_class) / sum(true_labels == positive_class)
  FPR_lda[i] <- sum(Yhat_lda == positive_class & true_labels == "Normal") / sum(true_labels == "Normal")
}

# Plot Error Rate vs. Threshold for LDA
plot(threshold_lda, err_rate_lda, type = "p", xlab = "Threshold (Probability of Extreme Weather)", ylab = "Error Rate",
     main = "LDA: Error Rate vs. Threshold", pch = 16, cex = 0.8)

```

```{r}
# Plot ROC Curve for LDA
plot(FPR_lda, TPR_lda, type = "p", xlab = "False Positive Rate", ylab = "True Positive Rate",
     main = "LDA: ROC Curve", pch = 16, cex = 0.8, xlim = c(0, 1), ylim = c(0, 1))
abline(0, 1, lty = 2, col = "red")
```

```{r}
threshold_qda <- seq(0, 1, 0.01)
TPR_qda <- FPR_qda <- err_rate_qda <- rep(0, length(threshold_qda))

for (i in seq_along(threshold_qda)) {
  Yhat_qda <- ifelse(qda_probs >= threshold_qda[[i]], positive_class, "Normal")
  Yhat_qda_factor <- factor(Yhat_qda, levels = c("Normal", "Extreme Weather"))
  err_rate_qda[i] <- mean(Yhat_qda_factor != true_labels)
  TPR_qda[i] <- sum(Yhat_qda == positive_class & true_labels == positive_class) / sum(true_labels == positive_class)
  FPR_qda[i] <- sum(Yhat_qda == positive_class & true_labels == "Normal") / sum(true_labels == "Normal")
}

```

```{r}
plot(threshold_qda, err_rate_qda, type = "p", xlab = "Threshold (Probability of Extreme Weather)", ylab = "Error Rate",
     main = "QDA: Error Rate vs. Threshold", pch = 16, cex = 0.8)

# Plot ROC Curve for QDA
plot(FPR_qda, TPR_qda, type = "p", xlab = "False Positive Rate", ylab = "True Positive Rate",
     main = "QDA: ROC Curve", pch = 16, cex = 0.8, xlim = c(0, 1), ylim = c(0, 1))
abline(0, 1, lty = 2, col = "red")
```

```{r}

best_idx_lda <- which.min(err_rate_lda)
best_threshold_lda <- threshold_lda[best_idx_lda]
best_TPR_lda <- TPR_lda[best_idx_lda]
best_FPR_lda <- FPR_lda[best_idx_lda]


best_idx_qda <- which.min(err_rate_qda)
best_threshold_qda <- threshold_qda[best_idx_qda]
best_TPR_qda <- TPR_qda[best_idx_qda]
best_FPR_qda <- FPR_qda[best_idx_qda]

```

```{r}
# LDA Results 
lda_results_df <- data.frame(
  Metric = c("Best Threshold",
             "TPR at Best Threshold",
             "FPR at Best Threshold"),
  Value = c(best_threshold_lda,
            best_TPR_lda,
            best_FPR_lda)
)

lda_results_df

# QDA Results 
qda_results_df <- data.frame(
  Metric = c("Best Threshold",
             "TPR at Best Threshold",
             "FPR at Best Threshold"),
  Value = c(best_threshold_qda,
            best_TPR_qda,
            best_FPR_qda)
)

qda_results_df
```

After evaluating the models, it seems that QDA performs better. While looking at the ROC curves and error rates, we found that the best threshold for LDA is 0.66 and QDA is 0.58.

## 2) Predictions for temperature patterns using Regression Methods

## Linear Regression

```{r}
# Select Data
regression_df <- weather_clean %>%
  dplyr::select(t2m, month, d2m_f, wind_speed_kmh, msl, sp, tp) %>%
  drop_na()
```

```{r}
lm_model <- lm(t2m ~ ., data = regression_df)
summary(lm_model)
```

The linear regression model achieved a high R² of 0.9165, indicating that over 91% of the variation in temperature (`t2m`) is explained by the selected predictors. All predictors were statistically significant, and the residual standard error was relatively low at 2.93, suggesting a good overall fit.

## Polynomial Regression

```{r}
poly_model <- lm(t2m ~ poly(month, 3), data = regression_df)
summary(poly_model)
```

The polynomial regression model used a 3rd-degree transformation of the `month` variable to model seasonal trends in temperature. While all terms were significant, the R² was lower at 0.6622, and the residual error was larger compared to the full linear model. This suggests that `month` alone, even with polynomial terms, is insufficient for accurate temperature prediction.

## Ridge/Lasso

```{r}
library(glmnet)

# Prepare data
X <- model.matrix(t2m ~ ., regression_df)[, -1]
y <- regression_df$t2m

# Ridge Regression (alpha = 0)
cv_ridge <- cv.glmnet(X, y, alpha = 0)
ridge_best_lambda <- cv_ridge$lambda.min
ridge_preds <- predict(cv_ridge, s = ridge_best_lambda, newx = X)
ridge_rmse <- sqrt(mean((ridge_preds - y)^2))
ridge_rmse

# Lasso Regression (alpha = 1)
cv_lasso <- cv.glmnet(X, y, alpha = 1)
lasso_best_lambda <- cv_lasso$lambda.min
lasso_preds <- predict(cv_lasso, s = lasso_best_lambda, newx = X)
lasso_rmse <- sqrt(mean((lasso_preds - y)^2))
lasso_rmse

# Coefficients
coef(cv_ridge, s = "lambda.min")
coef(cv_lasso, s = "lambda.min")

```

Ridge regression, which shrinks coefficients but retains all predictors, produced an RMSE of 4.65. While this is worse than the linear and lasso models, ridge was likely affected by collinearity among predictors and over-smoothing due to the penalty term.

Lasso regression yielded the best performance among the penalized methods, with an RMSE of 2.94 — very close to the unregularized linear model. It also performed variable selection, shrinking some coefficients more aggressively (e.g., `tp`) while still retaining key predictors, making it a strong choice for both accuracy and interpretability.

## Regression Models

```{r}
library(glmnet)
library(ggplot2)

# Prepare data
regression_df <- weather_clean %>%
  dplyr::select(t2m, month, d2m_f, wind_speed_kmh, msl, sp, tp) %>%
  drop_na()

X <- model.matrix(t2m ~ ., regression_df)[, -1]
y <- regression_df$t2m

# 1. Linear Regression
lm_model <- lm(t2m ~ ., data = regression_df)
lm_preds <- predict(lm_model)

# 2. Polynomial Regression (month as poly term)
poly_model <- lm(t2m ~ poly(month, 3), data = regression_df)
poly_preds <- predict(poly_model)

# 3. Ridge Regression
cv_ridge <- cv.glmnet(X, y, alpha = 0)
ridge_preds <- predict(cv_ridge, s = "lambda.min", newx = X)

# 4. Lasso Regression
cv_lasso <- cv.glmnet(X, y, alpha = 1)
lasso_preds <- predict(cv_lasso, s = "lambda.min", newx = X)

# Create long-format dataframe for plotting
plot_df <- data.frame(
  Actual = y,
  Linear = lm_preds,
  Polynomial = poly_preds,
  Ridge = as.vector(ridge_preds),
  Lasso = as.vector(lasso_preds)
) %>%
  pivot_longer(-Actual, names_to = "Model", values_to = "Predicted")

# Plot predicted vs. actual
ggplot(plot_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.3, size = 0.8) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  facet_wrap(~Model, scales = "free") +
  labs(title = "Predicted vs. Actual Temperature by Model",
       x = "Actual Temperature (K)", y = "Predicted Temperature (K)") +
  theme_minimal()
```

## Evaluation and Cross Validation

```{r}

# Classification model comparison
classification_results <- data.frame(
  Model = c("KNN (Unbalanced)", "KNN (Balanced)", "Logistic Regression", "LDA", "QDA"),
  Accuracy = c(0.985, 0.888, 0.064, 0.9197, 0.9399),
  Balanced_Accuracy = c(0.50, NA, NA, NA, NA),
  Notes = c(
    "Overestimates due to class imbalance",
    "More realistic; tuned with CV",
    "Underperforms despite significance",
    "Strong separation with linear boundaries",
    "Best classifier; handles non-linearity"
  )
)

knitr::kable(classification_results, caption = "Classification Model Comparison")

# Regression model comparison
regression_results <- data.frame(
  Model = c("Linear Regression", "Polynomial Regression", "Ridge Regression", "Lasso Regression"),
  RMSE = c(2.93, 5.88, 4.65, 2.94),
  Adjusted_R2 = c(0.9165, 0.6622, NA, NA),
  Notes = c(
    "Best overall fit; all predictors significant",
    "Captures seasonality; limited features",
    "Regularized; retains all predictors",
    "Sparse solution; similar to linear"
  )
)

regression_results
```

We evaluated multiple classification methods to predict extreme weather events, including K-Nearest Neighbors (KNN), logistic regression, LDA, and QDA. KNN initially appeared highly accurate (\>98%), but deeper inspection revealed this was due to severe class imbalance. After balancing the data, KNN's performance became more realistic, achieving \~88% accuracy and a balanced accuracy of 0.5. Logistic regression struggled even after rebalancing, with only 6.4% accuracy, indicating it could not capture complex decision boundaries. LDA and QDA, by contrast, performed significantly better, with classification accuracies of 91.97% and 93.99% respectively, showing that distribution-based models can better handle the underlying structure of extreme weather classification. Cross-validation was used to select the optimal number of neighbors in KNN and to ensure fair model comparison. These results highlight the importance of both class balance and cross-validation in evaluating classifier performance in imbalanced real-world datasets.

To evaluate model performance, we used RMSE (Root Mean Squared Error) and adjusted R² across four regression approaches: linear, polynomial, ridge, and lasso. The standard linear model performed best overall, with an adjusted R² of 0.9165 and a low residual error, indicating a strong linear relationship between temperature and the selected predictors. Polynomial regression, despite modeling seasonal trends with `month`, showed weaker performance due to its limited feature scope. Ridge regression used cross-validation to select the optimal penalty parameter (`λ`), but still underperformed with an RMSE of 4.65. Lasso regression also used cross-validation for tuning and achieved a much lower RMSE of 2.94 by automatically selecting relevant features. These results underscore the effectiveness of regularized models when handling collinearity or when model simplicity is preferred. Cross-validation played a key role in objectively comparing flexibility and prediction error, helping avoid overfitting.
